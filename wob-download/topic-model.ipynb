{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need NLTK\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to download some data from nltk\n",
    "import nltk\n",
    "# a gui screen will open to download relevant stuff\n",
    "# \n",
    "#nltk.download('punkt') # 'punkt' 'stopwords'\n",
    "#nltk.download('stopwords') # 'punkt' 'stopwords' 'wordnet' 'omw-1.4'\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus_root = \"./txt/\"\n",
    "file_ext = \"txt\"\n",
    "file_ids = [f for f in listdir(corpus_root) if isfile(join(corpus_root, f)) and f.lower().endswith(file_ext)]\n",
    "corpus = PlaintextCorpusReader(corpus_root, file_ids)\n",
    "print(\"The number of documents:\", len(corpus.fileids()))\n",
    "print(\"The number of sentences =\", len(corpus.sents()))\n",
    "print(\"The number of words =\", len([word for sentence in corpus.sents() for word in sentence]))\n",
    "#print(\"The number of characters =\", len([char for sentence in corpus.sents() for word in sentence for char in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus sentences\n",
    "import pickle\n",
    "\n",
    "pickleFile = open('corpus_sentences.pkl', 'wb')\n",
    "pickle.dump(corpus.sents(), pickleFile)\n",
    "pickleFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ada2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vect = CountVectorizer(max_df=2)\n",
    "# term document matrix (more efficient for large corpora)\n",
    "term_document_matrix = count_vect.fit_transform([corpus.raw(i) for i in file_ids])\n",
    "df_dtm = pd.DataFrame(term_document_matrix.toarray(), columns=count_vect.get_feature_names_out())\n",
    "df_dtm['file_ids'] = file_ids\n",
    "df_dtm=df_dtm.set_index('file_ids')\n",
    "df_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b00fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time\n",
    "df = pd.DataFrame(columns=['Text'])\n",
    "df['text'] = [corpus.raw(i) for i in file_ids]\n",
    "df['file_ids'] = file_ids\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# optional lemanize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# optional stemmer\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "all_stopwords = stopwords.words('english') + stopwords.words('dutch')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in all_stopwords]\n",
    "    # we need dutch tokenizers\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "df['clean_text'] = df['text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7799e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords.words('dutch')\n",
    "#stopwords.words('english')\n",
    "#all_stopwords = stopwords.words('english') + stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ad8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tmp result\n",
    "df.to_pickle(\"wob_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c772893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESTART here\n",
    "df = pd.read_pickle(\"wob_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean = df[['file_ids','clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean['clean_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdb711",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(' '.join(corpus_clean['clean_text']).split())\n",
    "topWords = freq.most_common(20)\n",
    "topWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic model\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import _stop_words\n",
    "# max_features limits the number of features to use\n",
    "vect = CountVectorizer(max_features=1000,ngram_range=(1,1),stop_words=['engilsh','dutch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df09fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a document term matrix\n",
    "dtm=vect.fit_transform(corpus_clean['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document term matrix\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea749a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many topics do we want to find\n",
    "lda=LatentDirichletAllocation(n_components=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc855c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "lda.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8782d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zit=pyLDAvis.sklearn.prepare(lda,dtm,vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(zit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
